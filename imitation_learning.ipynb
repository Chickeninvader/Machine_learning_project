{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U setuptools==65.5.0 pip==21\n",
    "!pip install gymnasium\n",
    "!pip install skrl\n",
    "!pip install stable_baselines3\n",
    "!pip install imitation\n",
    "!pip install highway-env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import highway_env\n",
    "import torch\n",
    "import numpy as np\n",
    "# import the skrl components to build the RL system\n",
    "from skrl.agents.torch.dqn import DQN, DQN_DEFAULT_CONFIG\n",
    "from skrl.envs.wrappers.torch import wrap_env\n",
    "from skrl.memories.torch import RandomMemory\n",
    "from skrl.trainers.torch import SequentialTrainer\n",
    "from skrl.utils import set_seed\n",
    "from skrl.utils.model_instantiators.torch import Shape, deterministic_model\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "\n",
    "from imitation.algorithms import bc\n",
    "from imitation.data import rollout\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "from imitation.policies.serialize import load_policy\n",
    "from imitation.util.util import make_vec_env\n",
    "import gymnasium as gym\n",
    "import highway_env\n",
    "from imitation.algorithms import bc\n",
    "from imitation.algorithms.dagger import SimpleDAggerTrainer\n",
    "from imitation.policies.serialize import load_policy\n",
    "from imitation.util.util import make_vec_env\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment is used is highway-v0, which I showed on the slide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;20m[skrl:INFO] Seed: 42\u001b[0m\n",
      "/Users/khoavo2003/anaconda3/envs/cr37/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:42: UserWarning: \u001b[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (5, 5)\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[38;20m[skrl:INFO] Environment class: gymnasium.core.Wrapper, gymnasium.utils.record_constructor.RecordConstructorArgs\u001b[0m\n",
      "\u001b[38;20m[skrl:INFO] Environment wrapper: Gymnasium\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# seed for reproducibility\n",
    "set_seed(42)  # e.g. `set_seed(42)` for fixed seed\n",
    "\n",
    "\n",
    "# load and wrap the gymnasium environment.\n",
    "# note: the environment version may change depending on the gymnasium version\n",
    "env = gym.make(\"highway-v0\", render_mode='rgb_array')\n",
    "env = wrap_env(env)\n",
    "\n",
    "device = env.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a memory as experience replay\n",
    "memory = RandomMemory(memory_size=50000, num_envs=env.num_envs, device=device, replacement=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# instantiate the agent's models (function approximators) using the model instantiator utility.\n",
    "# DQN requires 2 models, visit its documentation for more details\n",
    "# https://skrl.readthedocs.io/en/latest/api/agents/dqn.html#models\n",
    "models = {}\n",
    "models[\"q_network\"] = deterministic_model(observation_space=env.observation_space,\n",
    "                                          action_space=env.action_space,\n",
    "                                          device=device,\n",
    "                                          clip_actions=False,\n",
    "                                          input_shape=Shape.OBSERVATIONS,\n",
    "                                          hiddens=[64, 64],\n",
    "                                          hidden_activation=[\"relu\", \"relu\"],\n",
    "                                          output_shape=Shape.ACTIONS,\n",
    "                                          output_activation=None,\n",
    "                                          output_scale=1.0)\n",
    "models[\"target_q_network\"] = deterministic_model(observation_space=env.observation_space,\n",
    "                                                 action_space=env.action_space,\n",
    "                                                 device=device,\n",
    "                                                 clip_actions=False,\n",
    "                                                 input_shape=Shape.OBSERVATIONS,\n",
    "                                                 hiddens=[64, 64],\n",
    "                                                 hidden_activation=[\"relu\", \"relu\"],\n",
    "                                                 output_shape=Shape.ACTIONS,\n",
    "                                                 output_activation=None,\n",
    "                                                 output_scale=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize models' parameters (weights and biases)\n",
    "for model in models.values():\n",
    "    model.init_parameters(method_name=\"normal_\", mean=0.0, std=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure and instantiate the agent (visit its documentation to see all the options)\n",
    "# https://skrl.readthedocs.io/en/latest/api/agents/dqn.html\n",
    "#configuration-and-hyperparameters\n",
    "cfg = DQN_DEFAULT_CONFIG.copy()\n",
    "cfg[\"learning_starts\"] = 100\n",
    "cfg[\"exploration\"][\"final_epsilon\"] = 0.04\n",
    "cfg[\"exploration\"][\"timesteps\"] = 1500\n",
    "# logging to TensorBoard and write checkpoints (in timesteps)\n",
    "cfg[\"experiment\"][\"write_interval\"] = 1000\n",
    "cfg[\"experiment\"][\"checkpoint_interval\"] = 5000\n",
    "cfg[\"experiment\"][\"directory\"] = \"runs/torch/CartPole\"\n",
    "\n",
    "agent = DQN(models=models,\n",
    "            memory=memory,\n",
    "            cfg=cfg,\n",
    "            observation_space=env.observation_space,\n",
    "            action_space=env.action_space,\n",
    "            device=device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train example expert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.load(\"/content/runs/torch/highway-v0/23-09-26_16-06-57-103358_DQN/checkpoints/best_agent.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure and instantiate the RL trainer\n",
    "cfg_trainer = {\"timesteps\": 50000, \"headless\": True}\n",
    "trainer = SequentialTrainer(cfg=cfg_trainer, env=env, agents=[agent])\n",
    "\n",
    "# start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir='/content/runs/torch/highway-v0/23-09-26_17-05-02-586140_DQN' --port 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of episodes to collect transitions\n",
    "num_episodes = 3\n",
    "\n",
    "# Initialize lists to store transitions\n",
    "obs_list, acts_list, infos_list, next_obs_list, dones_list = [], [], [], [], []\n",
    "\n",
    "# Main loop to collect transitions from multiple episodes\n",
    "for _ in range(num_episodes):\n",
    "    # Reset the environment to the initial state\n",
    "    observation, info = env.reset()\n",
    "    done = False\n",
    "\n",
    "    # Episode-specific loop\n",
    "    while not done:\n",
    "        # Take a random action (replace this with your own agent's action)\n",
    "        action = env.action_space.sample()\n",
    "\n",
    "        # Step through the environment with the chosen action\n",
    "        next_observation, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "        # Append transitions to the lists\n",
    "        obs_list.append(observation)\n",
    "        acts_list.append(action)\n",
    "        infos_list.append(info)\n",
    "        next_obs_list.append(next_observation)\n",
    "        dones_list.append(done)\n",
    "\n",
    "        # Update the current observation\n",
    "        observation = next_observation\n",
    "\n",
    "# Close the environment when finished collecting transitions\n",
    "env.close()\n",
    "\n",
    "# Create a dictionary to store the transitions\n",
    "my_transitions = {\n",
    "    \"obs\": obs_list,\n",
    "    \"acts\": acts_list,\n",
    "    \"infos\": infos_list,\n",
    "    \"next_obs\": next_obs_list,\n",
    "    \"dones\": dones_list\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m8/f4qfc7j527x_gzcp82ry99940000gn/T/ipykernel_28646/522751877.py:5: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  obs=np.array(my_transitions[\"obs\"]),\n",
      "/var/folders/m8/f4qfc7j527x_gzcp82ry99940000gn/T/ipykernel_28646/522751877.py:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  obs=np.array(my_transitions[\"obs\"]),\n",
      "/var/folders/m8/f4qfc7j527x_gzcp82ry99940000gn/T/ipykernel_28646/522751877.py:8: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  next_obs=np.array(my_transitions[\"next_obs\"]),\n",
      "/var/folders/m8/f4qfc7j527x_gzcp82ry99940000gn/T/ipykernel_28646/522751877.py:8: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  next_obs=np.array(my_transitions[\"next_obs\"]),\n",
      "/var/folders/m8/f4qfc7j527x_gzcp82ry99940000gn/T/ipykernel_28646/522751877.py:9: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dones=np.array(my_transitions[\"dones\"], dtype=np.bool),\n",
      "/var/folders/m8/f4qfc7j527x_gzcp82ry99940000gn/T/ipykernel_28646/522751877.py:9: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  dones=np.array(my_transitions[\"dones\"], dtype=np.bool),\n",
      "/var/folders/m8/f4qfc7j527x_gzcp82ry99940000gn/T/ipykernel_28646/522751877.py:9: DeprecationWarning: setting an array element with a sequence. This was supported in some cases where the elements are arrays with a single element. For example `np.array([1, np.array([2])], dtype=int)`. In the future this will raise the same ValueError as `np.array([1, [2]], dtype=int)`.\n",
      "  dones=np.array(my_transitions[\"dones\"], dtype=np.bool),\n"
     ]
    }
   ],
   "source": [
    "from imitation.data import types\n",
    "\n",
    "def load_custom_transitions(my_transitions):\n",
    "    transitions = types.Transitions(\n",
    "        obs=np.array(my_transitions[\"obs\"]),\n",
    "        acts=np.array(my_transitions[\"acts\"], dtype=np.int32),\n",
    "        infos=my_transitions[\"infos\"],\n",
    "        next_obs=np.array(my_transitions[\"next_obs\"]),\n",
    "        dones=np.array(my_transitions[\"dones\"], dtype=np.bool),\n",
    "    )\n",
    "    return transitions\n",
    "\n",
    "# Load your custom transitions\n",
    "custom_transitions = load_custom_transitions(my_transitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the new environment since the old environment is wrapped, thus cannot be use in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/khoavo2003/anaconda3/envs/cr37/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:42: UserWarning: \u001b[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (5, 5)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env1 = gym.make('highway-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(0)\n",
    "bc_trainer = bc.BC(\n",
    "    observation_space=env1.observation_space,\n",
    "    action_space=env1.action_space,\n",
    "    demonstrations=custom_transitions,\n",
    "    rng=rng,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/khoavo2003/anaconda3/envs/cr37/lib/python3.11/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward before training: 23.385851586858433\n"
     ]
    }
   ],
   "source": [
    "reward, _ = evaluate_policy(\n",
    "    bc_trainer.policy,  # type: ignore[arg-type]\n",
    "    env1,\n",
    "    n_eval_episodes=3,\n",
    "    # render=True,  # comment out to speed up\n",
    ")\n",
    "print(f\"Reward before training: {reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a policy using Behavior Cloning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0batch [00:00, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| batch_size        | 32       |\n",
      "| bc/               |          |\n",
      "|    batch          | 0        |\n",
      "|    ent_loss       | -0.00161 |\n",
      "|    entropy        | 1.61     |\n",
      "|    epoch          | 0        |\n",
      "|    l2_loss        | 0        |\n",
      "|    l2_norm        | 115      |\n",
      "|    loss           | 1.61     |\n",
      "|    neglogp        | 1.61     |\n",
      "|    prob_true_act  | 0.2      |\n",
      "|    samples_so_far | 32       |\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100batch [00:00, 114.04batch/s]\n",
      "/Users/khoavo2003/anaconda3/envs/cr37/lib/python3.11/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward after training: 22.374947667121887\n"
     ]
    }
   ],
   "source": [
    "print(\"Training a policy using Behavior Cloning\")\n",
    "bc_trainer.train(n_epochs=100)\n",
    "\n",
    "reward, _ = evaluate_policy(\n",
    "    bc_trainer.policy,  # type: ignore[arg-type]\n",
    "    env1,\n",
    "    n_eval_episodes=3,\n",
    ")\n",
    "print(f\"Reward after training: {reward}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cr37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
