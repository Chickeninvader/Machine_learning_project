{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U setuptools==65.5.0 pip==21\n",
    "!pip install gymnasium\n",
    "!pip install skrl\n",
    "!pip install stable_baselines3\n",
    "!pip install imitation\n",
    "!pip install highway-env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import highway_env\n",
    "import torch\n",
    "import numpy as np\n",
    "# import the skrl components to build the RL system\n",
    "from skrl.agents.torch.dqn import DQN, DQN_DEFAULT_CONFIG\n",
    "from skrl.envs.wrappers.torch import wrap_env\n",
    "from skrl.memories.torch import RandomMemory\n",
    "from skrl.trainers.torch import SequentialTrainer\n",
    "from skrl.utils import set_seed\n",
    "from skrl.utils.model_instantiators.torch import Shape, deterministic_model\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "\n",
    "from imitation.algorithms import bc\n",
    "from imitation.data import rollout\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "from imitation.policies.serialize import load_policy\n",
    "from imitation.util.util import make_vec_env\n",
    "import gymnasium as gym\n",
    "import highway_env\n",
    "from imitation.algorithms import bc\n",
    "from imitation.algorithms.dagger import SimpleDAggerTrainer\n",
    "from imitation.policies.serialize import load_policy\n",
    "from imitation.util.util import make_vec_env\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment is used is highway-v0, which I showed on the slide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;20m[skrl:INFO] Seed: 42\u001b[0m\n",
      "/Users/khoavo2003/anaconda3/envs/cr37/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:42: UserWarning: \u001b[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (5, 5)\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[38;20m[skrl:INFO] Environment class: gymnasium.core.Wrapper, gymnasium.utils.record_constructor.RecordConstructorArgs\u001b[0m\n",
      "\u001b[38;20m[skrl:INFO] Environment wrapper: Gymnasium\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# seed for reproducibility\n",
    "set_seed(42)  # e.g. `set_seed(42)` for fixed seed\n",
    "\n",
    "\n",
    "# load and wrap the gymnasium environment.\n",
    "# note: the environment version may change depending on the gymnasium version\n",
    "env = gym.make(\"highway-v0\", render_mode='rgb_array')\n",
    "env = wrap_env(env)\n",
    "\n",
    "device = env.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a memory as experience replay\n",
    "memory = RandomMemory(memory_size=50000, num_envs=env.num_envs, device=device, replacement=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# instantiate the agent's models (function approximators) using the model instantiator utility.\n",
    "# DQN requires 2 models, visit its documentation for more details\n",
    "# https://skrl.readthedocs.io/en/latest/api/agents/dqn.html#models\n",
    "models = {}\n",
    "models[\"q_network\"] = deterministic_model(observation_space=env.observation_space,\n",
    "                                          action_space=env.action_space,\n",
    "                                          device=device,\n",
    "                                          clip_actions=False,\n",
    "                                          input_shape=Shape.OBSERVATIONS,\n",
    "                                          hiddens=[64, 64],\n",
    "                                          hidden_activation=[\"relu\", \"relu\"],\n",
    "                                          output_shape=Shape.ACTIONS,\n",
    "                                          output_activation=None,\n",
    "                                          output_scale=1.0)\n",
    "models[\"target_q_network\"] = deterministic_model(observation_space=env.observation_space,\n",
    "                                                 action_space=env.action_space,\n",
    "                                                 device=device,\n",
    "                                                 clip_actions=False,\n",
    "                                                 input_shape=Shape.OBSERVATIONS,\n",
    "                                                 hiddens=[64, 64],\n",
    "                                                 hidden_activation=[\"relu\", \"relu\"],\n",
    "                                                 output_shape=Shape.ACTIONS,\n",
    "                                                 output_activation=None,\n",
    "                                                 output_scale=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize models' parameters (weights and biases)\n",
    "for model in models.values():\n",
    "    model.init_parameters(method_name=\"normal_\", mean=0.0, std=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure and instantiate the agent (visit its documentation to see all the options)\n",
    "# https://skrl.readthedocs.io/en/latest/api/agents/dqn.html\n",
    "#configuration-and-hyperparameters\n",
    "cfg = DQN_DEFAULT_CONFIG.copy()\n",
    "cfg[\"learning_starts\"] = 100\n",
    "cfg[\"exploration\"][\"final_epsilon\"] = 0.04\n",
    "cfg[\"exploration\"][\"timesteps\"] = 1500\n",
    "# logging to TensorBoard and write checkpoints (in timesteps)\n",
    "cfg[\"experiment\"][\"write_interval\"] = 1000\n",
    "cfg[\"experiment\"][\"checkpoint_interval\"] = 5000\n",
    "cfg[\"experiment\"][\"directory\"] = \"runs/torch/CartPole\"\n",
    "\n",
    "agent = DQN(models=models,\n",
    "            memory=memory,\n",
    "            cfg=cfg,\n",
    "            observation_space=env.observation_space,\n",
    "            action_space=env.action_space,\n",
    "            device=device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train example expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 9/50000 [00:03<5:12:51,  2.66it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m trainer \u001b[39m=\u001b[39m SequentialTrainer(cfg\u001b[39m=\u001b[39mcfg_trainer, env\u001b[39m=\u001b[39menv, agents\u001b[39m=\u001b[39m[agent])\n\u001b[1;32m      5\u001b[0m \u001b[39m# start training\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m trainer\u001b[39m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/cs224/imitation/skrl/skrl/trainers/torch/sequential.py:77\u001b[0m, in \u001b[0;36mSequentialTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_simultaneous_agents \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m     75\u001b[0m     \u001b[39m# single-agent\u001b[39;00m\n\u001b[1;32m     76\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mnum_agents \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 77\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msingle_agent_train()\n\u001b[1;32m     78\u001b[0m     \u001b[39m# multi-agent\u001b[39;00m\n\u001b[1;32m     79\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmulti_agent_train()\n",
      "File \u001b[0;32m~/cs224/imitation/skrl/skrl/trainers/torch/base.py:175\u001b[0m, in \u001b[0;36mTrainer.single_agent_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    172\u001b[0m actions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magents\u001b[39m.\u001b[39mact(states, timestep\u001b[39m=\u001b[39mtimestep, timesteps\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimesteps)[\u001b[39m0\u001b[39m]\n\u001b[1;32m    174\u001b[0m \u001b[39m# step the environments\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m next_states, rewards, terminated, truncated, infos \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mstep(actions)\n\u001b[1;32m    177\u001b[0m \u001b[39m# render scene\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheadless:\n",
      "File \u001b[0;32m~/cs224/imitation/skrl/skrl/envs/wrappers/torch/gymnasium_envs.py:124\u001b[0m, in \u001b[0;36mGymnasiumWrapper.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, actions: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor, torch\u001b[39m.\u001b[39mTensor, torch\u001b[39m.\u001b[39mTensor, torch\u001b[39m.\u001b[39mTensor, Any]:\n\u001b[1;32m    116\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Perform a step in the environment\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \n\u001b[1;32m    118\u001b[0m \u001b[39m    :param actions: The actions to perform\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[39m    :rtype: tuple of torch.Tensor and any other info\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m     observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_env\u001b[39m.\u001b[39mstep(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tensor_to_action(actions))\n\u001b[1;32m    126\u001b[0m     \u001b[39m# convert response to torch\u001b[39;00m\n\u001b[1;32m    127\u001b[0m     observation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_observation_to_tensor(observation)\n",
      "File \u001b[0;32m~/anaconda3/envs/cr37/lib/python3.11/site-packages/gymnasium/wrappers/order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[1;32m     55\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling env.reset()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mstep(action)\n",
      "File \u001b[0;32m~/anaconda3/envs/cr37/lib/python3.11/site-packages/gymnasium/wrappers/env_checker.py:49\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[39mreturn\u001b[39;00m env_step_passive_checker(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv, action)\n\u001b[1;32m     48\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 49\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mstep(action)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/highway_env/envs/common/abstract.py:235\u001b[0m, in \u001b[0;36mAbstractEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mThe road and vehicle must be initialized in the environment implementation\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    234\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtime \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig[\u001b[39m\"\u001b[39m\u001b[39mpolicy_frequency\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m--> 235\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_simulate(action)\n\u001b[1;32m    237\u001b[0m obs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation_type\u001b[39m.\u001b[39mobserve()\n\u001b[1;32m    238\u001b[0m reward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reward(action)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/highway_env/envs/common/abstract.py:258\u001b[0m, in \u001b[0;36mAbstractEnv._simulate\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_type\u001b[39m.\u001b[39mact(action)\n\u001b[1;32m    257\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroad\u001b[39m.\u001b[39mact()\n\u001b[0;32m--> 258\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroad\u001b[39m.\u001b[39mstep(\u001b[39m1\u001b[39m \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig[\u001b[39m\"\u001b[39m\u001b[39msimulation_frequency\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    259\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    261\u001b[0m \u001b[39m# Automatically render intermediate simulation steps if a viewer has been launched\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \u001b[39m# Ignored if the rendering is done offscreen\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/highway_env/road/road.py:349\u001b[0m, in \u001b[0;36mRoad.step\u001b[0;34m(self, dt)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[39mfor\u001b[39;00m i, vehicle \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvehicles):\n\u001b[1;32m    348\u001b[0m     \u001b[39mfor\u001b[39;00m other \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvehicles[i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m:]:\n\u001b[0;32m--> 349\u001b[0m         vehicle\u001b[39m.\u001b[39mhandle_collisions(other, dt)\n\u001b[1;32m    350\u001b[0m     \u001b[39mfor\u001b[39;00m other \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjects:\n\u001b[1;32m    351\u001b[0m         vehicle\u001b[39m.\u001b[39mhandle_collisions(other, dt)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/highway_env/vehicle/objects.py:82\u001b[0m, in \u001b[0;36mRoadObject.handle_collisions\u001b[0;34m(self, other, dt)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcollidable \u001b[39mand\u001b[39;00m other\u001b[39m.\u001b[39mcollidable):\n\u001b[1;32m     81\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m intersecting, will_intersect, transition \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_colliding(other, dt)\n\u001b[1;32m     83\u001b[0m \u001b[39mif\u001b[39;00m will_intersect:\n\u001b[1;32m     84\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msolid \u001b[39mand\u001b[39;00m other\u001b[39m.\u001b[39msolid:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/highway_env/vehicle/objects.py:103\u001b[0m, in \u001b[0;36mRoadObject._is_colliding\u001b[0;34m(self, other, dt)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_is_colliding\u001b[39m(\u001b[39mself\u001b[39m, other, dt):\n\u001b[1;32m    102\u001b[0m     \u001b[39m# Fast spherical pre-check\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m     \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39mnorm(other\u001b[39m.\u001b[39mposition \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition) \u001b[39m>\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdiagonal \u001b[39m+\u001b[39m other\u001b[39m.\u001b[39mdiagonal) \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspeed \u001b[39m*\u001b[39m dt:\n\u001b[1;32m    104\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39mFalse\u001b[39;00m, np\u001b[39m.\u001b[39mzeros(\u001b[39m2\u001b[39m,)\n\u001b[1;32m    105\u001b[0m     \u001b[39m# Accurate rectangular check\u001b[39;00m\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mnorm\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/cr37/lib/python3.11/site-packages/numpy/linalg/linalg.py:2526\u001b[0m, in \u001b[0;36mnorm\u001b[0;34m(x, ord, axis, keepdims)\u001b[0m\n\u001b[1;32m   2524\u001b[0m     sqnorm \u001b[39m=\u001b[39m x_real\u001b[39m.\u001b[39mdot(x_real) \u001b[39m+\u001b[39m x_imag\u001b[39m.\u001b[39mdot(x_imag)\n\u001b[1;32m   2525\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2526\u001b[0m     sqnorm \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mdot(x)\n\u001b[1;32m   2527\u001b[0m ret \u001b[39m=\u001b[39m sqrt(sqnorm)\n\u001b[1;32m   2528\u001b[0m \u001b[39mif\u001b[39;00m keepdims:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# configure and instantiate the RL trainer\n",
    "cfg_trainer = {\"timesteps\": 50000, \"headless\": True}\n",
    "trainer = SequentialTrainer(cfg=cfg_trainer, env=env, agents=[agent])\n",
    "\n",
    "# start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of episodes to collect transitions\n",
    "num_episodes = 3\n",
    "\n",
    "# Initialize lists to store transitions\n",
    "obs_list, acts_list, infos_list, next_obs_list, dones_list = [], [], [], [], []\n",
    "\n",
    "# Main loop to collect transitions from multiple episodes\n",
    "for _ in range(num_episodes):\n",
    "    # Reset the environment to the initial state\n",
    "    observation, info = env.reset()\n",
    "    done = False\n",
    "\n",
    "    # Episode-specific loop\n",
    "    while not done:\n",
    "        # Take a random action (replace this with your own agent's action)\n",
    "        action = env.action_space.sample()\n",
    "\n",
    "        # Step through the environment with the chosen action\n",
    "        next_observation, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "        # Append transitions to the lists\n",
    "        obs_list.append(observation)\n",
    "        acts_list.append(action)\n",
    "        infos_list.append(info)\n",
    "        next_obs_list.append(next_observation)\n",
    "        dones_list.append(done)\n",
    "\n",
    "        # Update the current observation\n",
    "        observation = next_observation\n",
    "\n",
    "# Close the environment when finished collecting transitions\n",
    "env.close()\n",
    "\n",
    "# Create a dictionary to store the transitions\n",
    "my_transitions = {\n",
    "    \"obs\": obs_list,\n",
    "    \"acts\": acts_list,\n",
    "    \"infos\": infos_list,\n",
    "    \"next_obs\": next_obs_list,\n",
    "    \"dones\": dones_list\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m8/f4qfc7j527x_gzcp82ry99940000gn/T/ipykernel_28646/522751877.py:5: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  obs=np.array(my_transitions[\"obs\"]),\n",
      "/var/folders/m8/f4qfc7j527x_gzcp82ry99940000gn/T/ipykernel_28646/522751877.py:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  obs=np.array(my_transitions[\"obs\"]),\n",
      "/var/folders/m8/f4qfc7j527x_gzcp82ry99940000gn/T/ipykernel_28646/522751877.py:8: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  next_obs=np.array(my_transitions[\"next_obs\"]),\n",
      "/var/folders/m8/f4qfc7j527x_gzcp82ry99940000gn/T/ipykernel_28646/522751877.py:8: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  next_obs=np.array(my_transitions[\"next_obs\"]),\n",
      "/var/folders/m8/f4qfc7j527x_gzcp82ry99940000gn/T/ipykernel_28646/522751877.py:9: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dones=np.array(my_transitions[\"dones\"], dtype=np.bool),\n",
      "/var/folders/m8/f4qfc7j527x_gzcp82ry99940000gn/T/ipykernel_28646/522751877.py:9: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  dones=np.array(my_transitions[\"dones\"], dtype=np.bool),\n",
      "/var/folders/m8/f4qfc7j527x_gzcp82ry99940000gn/T/ipykernel_28646/522751877.py:9: DeprecationWarning: setting an array element with a sequence. This was supported in some cases where the elements are arrays with a single element. For example `np.array([1, np.array([2])], dtype=int)`. In the future this will raise the same ValueError as `np.array([1, [2]], dtype=int)`.\n",
      "  dones=np.array(my_transitions[\"dones\"], dtype=np.bool),\n"
     ]
    }
   ],
   "source": [
    "from imitation.data import types\n",
    "\n",
    "def load_custom_transitions(my_transitions):\n",
    "    transitions = types.Transitions(\n",
    "        obs=np.array(my_transitions[\"obs\"]),\n",
    "        acts=np.array(my_transitions[\"acts\"], dtype=np.int32),\n",
    "        infos=my_transitions[\"infos\"],\n",
    "        next_obs=np.array(my_transitions[\"next_obs\"]),\n",
    "        dones=np.array(my_transitions[\"dones\"], dtype=np.bool),\n",
    "    )\n",
    "    return transitions\n",
    "\n",
    "# Load your custom transitions\n",
    "custom_transitions = load_custom_transitions(my_transitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the new environment since the old environment is wrapped, thus cannot be use in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/khoavo2003/anaconda3/envs/cr37/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:42: UserWarning: \u001b[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (5, 5)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env1 = gym.make('highway-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(0)\n",
    "bc_trainer = bc.BC(\n",
    "    observation_space=env1.observation_space,\n",
    "    action_space=env1.action_space,\n",
    "    demonstrations=custom_transitions,\n",
    "    rng=rng,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/khoavo2003/anaconda3/envs/cr37/lib/python3.11/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward before training: 23.385851586858433\n"
     ]
    }
   ],
   "source": [
    "reward, _ = evaluate_policy(\n",
    "    bc_trainer.policy,  # type: ignore[arg-type]\n",
    "    env1,\n",
    "    n_eval_episodes=3,\n",
    "    # render=True,  # comment out to speed up\n",
    ")\n",
    "print(f\"Reward before training: {reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a policy using Behavior Cloning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0batch [00:00, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| batch_size        | 32       |\n",
      "| bc/               |          |\n",
      "|    batch          | 0        |\n",
      "|    ent_loss       | -0.00161 |\n",
      "|    entropy        | 1.61     |\n",
      "|    epoch          | 0        |\n",
      "|    l2_loss        | 0        |\n",
      "|    l2_norm        | 115      |\n",
      "|    loss           | 1.61     |\n",
      "|    neglogp        | 1.61     |\n",
      "|    prob_true_act  | 0.2      |\n",
      "|    samples_so_far | 32       |\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100batch [00:00, 114.04batch/s]\n",
      "/Users/khoavo2003/anaconda3/envs/cr37/lib/python3.11/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward after training: 22.374947667121887\n"
     ]
    }
   ],
   "source": [
    "print(\"Training a policy using Behavior Cloning\")\n",
    "bc_trainer.train(n_epochs=100)\n",
    "\n",
    "reward, _ = evaluate_policy(\n",
    "    bc_trainer.policy,  # type: ignore[arg-type]\n",
    "    env1,\n",
    "    n_eval_episodes=3,\n",
    ")\n",
    "print(f\"Reward after training: {reward}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cr37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
