{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from ray import tune, train\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.algorithms.a2c import A2C, A2CConfig\n",
    "\n",
    "import gymnasium as gym\n",
    "from ray.tune.registry import register_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config training parameters\n",
    "train_config = {\n",
    "    \"env\": \"CartPole-v1\", # MyCustomEnv_v0,\n",
    "    \"framework\": \"torch\",\n",
    "    \"num_workers\": 2,\n",
    "    \"num_cpus_per_worker\": 3,\n",
    "    \"model\": {\n",
    "        \"fcnet_hiddens\": [512, 512, 256],\n",
    "        \"fcnet_activation\": \"relu\",\n",
    "    },\n",
    "    \"lr\": tune.grid_search([0.001,0.0001]),  \n",
    "    \"optimization\": {\n",
    "        \"optimizer\": \"adam\",\n",
    "        \"adam_epsilon\": 1e-8,\n",
    "        \"adam_beta1\": 0.9,\n",
    "        \"adam_beta2\": 0.999,\n",
    "    },  \n",
    "    \"gamma\": 0.99,\n",
    "    \"num_sgd_iter\": 10,  \n",
    "    \"sgd_minibatch_size\": 500, \n",
    "    \"rollout_fragment_length\": 500,\n",
    "    \"train_batch_size\": 4000,\n",
    "    \"prioritized_replay\": True,\n",
    "    \"prioritized_replay_alpha\": 0.6,\n",
    "    \"prioritized_replay_beta\": 0.4, \n",
    "    \"buffer_size\": 500000,\n",
    "    \"stop\": {\"episodes_total\": 5000000},\n",
    "    \"exploration_config\": {},\n",
    "}\n",
    "stop_criteria = {\"episode_reward_mean\": 400}\n",
    "\n",
    "# start to train\n",
    "results = tune.run(\n",
    "    A2C, \n",
    "    config=train_config,\n",
    "    stop=stop_criteria,\n",
    "    verbose=1,\n",
    "    checkpoint_freq=1,\n",
    "    keep_checkpoints_num=1,\n",
    "    checkpoint_score_attr='training_iteration',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = A2CConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ray.rllib.algorithms.a2c.a2c.A2CConfig at 0x29e29cc10>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_config = {\n",
    "  \"gamma\": 0.99, # discount factor\n",
    "  \"lr\": tune.grid_search([0.001,0.0001]),\n",
    "  \"grad_clip\":\n",
    "  \"grad_clip_by\":\n",
    "  \"train_batch_size\": 512,\n",
    "  \"model\":  {\n",
    "    \n",
    "  },\n",
    "  \"optimizer\":,\n",
    "}\n",
    "\n",
    "environment_config = {\n",
    "  \"env\": \"CartPole-v1\",\n",
    "  \"render_env\": False, # If True, try to render the environment on the local worker or on worker 1 \n",
    "  \"auto_wrap_old_gym_envs\":\n",
    "}\n",
    "\n",
    "algorithm_config = {\n",
    "  \"framework\": \"torch\",\n",
    "\n",
    "\n",
    "}\n",
    "\n",
    "rollout_config = {\n",
    "  \"num_rollout_workers\":,\n",
    "  \"num_envs_per_worker\":,\n",
    "  \"sample_collector\":, #Override the SampleCollector base class to implement your own collection/buffering/retrieval logic.\n",
    "}\n",
    "\n",
    "evaluation_config = {\n",
    "  \"evaluation_interval\":,\n",
    "  \"evaluation_duration\":,\n",
    "  \"evaluation_duration_utit\":,\n",
    "  \"custom_evaluation_function\":,\n",
    "}\n",
    "\n",
    "reporting_config = {\n",
    "\n",
    "}\n",
    "\n",
    "checkpointing_config = {\n",
    "  \n",
    "}\n",
    "\n",
    "callback_config = {\n",
    "\n",
    "}\n",
    "\n",
    "resources_config = {\n",
    "  \"num_gpus\":,\n",
    "  \"num_cpus_per_worker\":,\n",
    "  \"num_gpus_per_worker\":,\n",
    "  \"num_learner_workers\":,\n",
    "  \"num_cpus_per_learner_worker\":,\n",
    "  \"num_gpus_per_learner_worker\":,\n",
    "  \"local_gpu_idx\":,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling Training with RLlib: Best Practices\n",
    "\n",
    "## 1. Sample-Efficient Off-Policy Algorithms (e.g., DQN, SAC)\n",
    "\n",
    "When the environment is slow and cannot be replicated, especially in scenarios involving physical systems, sample-efficient off-policy algorithms like DQN or SAC are recommended. These algorithms default to `num_workers: 0` for single-process operation. Ensure to set `num_gpus: 1` if GPU usage is desired. Additionally, consider exploring batch RL training with the offline data API.\n",
    "\n",
    "## 2. Time-Efficient Algorithms (e.g., PPO, IMPALA, APEX)\n",
    "\n",
    "For fast environments and small models (typical in RL scenarios), opt for time-efficient algorithms like PPO, IMPALA, or APEX. These algorithms can be efficiently scaled by increasing the `num_workers` to add rollout workers. Enabling vectorization for inference can further enhance efficiency. Don't forget to set `num_gpus: 1` if GPU acceleration is desired. If the learner becomes a bottleneck, leverage multiple GPUs for learning by setting `num_gpus > 1`.\n",
    "\n",
    "## 3. Compute-Intensive Models and GPU Allocation\n",
    "\n",
    "In scenarios where the model demands significant computational power (e.g., large deep residual networks) and inference becomes the bottleneck, consider allocating GPUs to workers. This can be achieved by setting `num_gpus_per_worker: 1`. If you possess only a single GPU, consider setting `num_workers: 0` to utilize the learner GPU for inference. For optimal GPU utilization, use a small number of GPU workers and a large number of environments per worker.\n",
    "\n",
    "## 4. Scaling with Remote Worker Environments and Async Batching\n",
    "\n",
    "When both the model and environment are compute-intensive, enabling remote worker environments with async batching can significantly enhance efficiency. Set `remote_worker_envs: True` and optionally configure `remote_env_batch_wait_ms`. This configuration batches inference on GPUs in the rollout workers while allowing environments to run asynchronously in separate actors, resembling the SEED architecture. To maximize GPU utilization, fine-tune the number of workers and environments per worker. If your environment requires GPUs to function or multi-node SGD is necessary, consider exploring DD-PPO.\n",
    "\n",
    "These guidelines provide valuable insights into optimizing training efficiency based on the specific characteristics of your environment and model in RLlib.\n",
    "\n",
    "\n",
    "In case you are using lots of workers (num_workers >> 10) and you observe worker failures for whatever reasons, which normally interrupt your RLlib training runs, consider using the config settings ignore_worker_failures=True, recreate_failed_workers=True, or restart_failed_sub_environments=True:\n",
    "\n",
    "ignore_worker_failures: When set to True, your Algorithm will not crash due to a single worker error but continue for as long as there is at least one functional worker remaining. recreate_failed_workers: When set to True, your Algorithm will attempt to replace/recreate any failed worker(s) with newly created one(s). This way, your number of workers will never decrease, even if some of them fail from time to time. restart_failed_sub_environments: When set to True and there is a failure in one of the vectorized sub-environments in one of your workers, the worker will try to recreate only the failed sub-environment and re-integrate the newly created one into your vectorized env stack on that worker.\n",
    "\n",
    "Note that only one of ignore_worker_failures or recreate_failed_workers may be set to True (they are mutually exclusive settings). However, you can combine each of these with the restart_failed_sub_environments=True setting. Using these options will make your training runs much more stable and more robust against occasional OOM or other similar “once in a while” errors on your workers themselves or inside your environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"monitor\": true config can be used to save Gym episode videos to the result dir. For example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cr37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
